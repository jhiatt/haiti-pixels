---
title: "Disaster Relief Project: Part I"
author: "Jordan Hiatt"
date: "`r format(Sys.Date(), '%b %d, %Y')`"
output:
  html_document:
    number_sections: true    
    toc: true
    toc_float: true
    theme: cosmo
    highlight: espresso    
# You can make the format personal - this will get you started:  
# https://bookdown.org/yihui/rmarkdown/html-document.html#appearance_and_style    
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5.5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=TRUE,      # show R warnings
                      message=TRUE)      # show R messages
```

<!--- Change font sizes (or other css modifications) --->
<style>
h1.title {
  font-size: 2.2em; /* Title font size */
}
h1 {
  font-size: 2em;   /* Header 1 font size */
}
h2 {
  font-size: 1.5em;
}
h3 { 
  font-size: 1.2em;
}
pre {
  font-size: 0.8em;  /* Code and R output font size */
}
</style>



**SYS 6018 | Spring 2021 | University of Virginia **

*******************************************

# Introduction 

Tell the reader what this project is about. Motivation. 

# Training Data / EDA

Load data, explore data, etc. 

```{r load-packages, warning=FALSE, message=FALSE}
# Load Required Packages
library(tidyverse)
library(broom)
library(caret)
library(MASS)
library(purrr)
```

# Model Training

## Set-up 

First, I loaded the data and did some digging on the different classes. A huge majority of our observations are not classified as Blue Tarps. We are not intersted in further defining the non-tarp data at this point so I grouped the non-tarp data into one class and saved it to another column to maintain the integrity of the original data, in case it is useful down the road.  After colapsing classes, only 3.3% of our data is classified as blue tarp, with the rest belonging to the "Other" class.

```{r load-data, cache=TRUE}
# load in csv
data <- read.csv('HaitiPixels.csv',colClasses=c("factor",rep("numeric",3))) # for help with colClasses and rep() https://stackoverflow.com/questions/2805357/specifying-colclasses-in-the-read-csv

# gather general csv data
data %>% dim()
data %>% filter(is.na(data))
# classes
data$Class %>% contrasts()
data %>% filter(Class=='Blue Tarp') %>% dim()
data %>% filter(Class=='Rooftop') %>% dim()
data %>% filter(Class=='Soil') %>% dim()
data %>% filter(Class=='Various Non-Tarp') %>% dim()
data %>% filter(Class=='Vegetation') %>% dim()

# Combine classes
data <- data %>% mutate(Class.cl = ifelse(Class=='Blue Tarp','Blue Tarp','Other'))
data$Class.cl <- data$Class.cl %>% as_factor()
data$Class.cl %>% contrasts()

# what percentage of our data is 'Blue Tarp'
bt <- data %>% filter(Class.cl=='Blue Tarp') %>% dim()
ot <- data %>% filter(Class.cl!='Blue Tarp') %>% dim()
bt[1]/ot[1] # 3.3%
```



```{r holdout, echo=FALSE, eval=FALSE}
# Next, I split out a subset of the data to use to test the finalized, cross validated data. Such a small portion of our data is blue tarp data that I am concerned we might not have enough blue tarp data for our test set. I did some calculations and found that only 0.03% of our data is blue tarp data. When deciding how much our holdout should be, my key priority is having enough blue tarp data to confirm that the testing works. I will start with 500 and then if there is time and processing capacity I can try a smaller amount to give more data for training. I am aware that there are other ways of allocating data to ensure you have important data in each set but I am wary of taking away too many blue tarps from training data or creating overfitting by including points in both.

After splitting out train and test data sets, I checked to make sure data was not lost and appeared to have the same balance on the whole.
## split into train and holdout
# what % of data is classified as blue tarp?
bt <- data %>% filter(Class.cl=='Blue Tarp') %>% dim()
ot <- data %>% filter(Class.cl!='Blue Tarp') %>% dim()
bt[1]/ot[1] # 3.3%

## !!!!!!!!!!!!!!!!!!!!!!!! remove this until section 2
# to get 500 blue tarps in our holdout set, how much data should we hold out?
500/.033 #15,151 about a quarter of our data
# what about 250?
250/.033
# we can start by holding out 1 quarter of the data and then see if our resluts are strong enough.

# holdout a quarter of our data
set.seed(42)
holdout_index <- sample(1:dim(data)[1], size = (dim(data)[1])/4, replace = FALSE)
data.test <- data[holdout_index,]
ts <- data.test %>% dim() #15810

`%notin%` <- Negate(`%in%`) #https://www.r-bloggers.com/2018/07/the-notin-operator/
data.train <- data %>% rowid_to_column() %>% filter(rowid %notin% holdout_index)
tr <- data.train %>% dim() #47431

ts[1]+tr[1]

ts.bl <- data.test %>% filter(Class.cl == "Blue Tarp") %>% dim()
ts.bl
tr.bl <- data.train %>% filter(Class.cl == "Blue Tarp") %>%  dim()
tr.bl
ts.bl + tr.bl
data %>% filter(Class.cl == "Blue Tarp") %>% dim()
```

I decided to create my own 10-fold split so I would more easily have access to any data that I wanted. This also gives me the advantage of having the same data for every methodology where it is used. Unfortunatly, due to time constraints not all methods used this cross validation split. Currently, this loop isn't very efficient so there is probably a better way, but the upside is it only has to run once and I should be able to use this on most of my models. Because we are dealing with three variables that represent different aspects of the color spectrum, I feel that this data is already pretty uniform, so I decided against any kind of scaling or parameter selection.

```{r 10-fold, cache=TRUE}
## set up 10-fold cross-validation
set.seed(42)
# First we get the number of items in each fold, then we get an index number cutt off for each fold if we were to number observations 1 to n
foldcount <- floor(dim(data)[1]/10) # this should give us the correct number of observations in each fold
# we need to make sure there is no top end or the last observation would be missed due to rounding, so we stop at 9
cuttoff_list <- list()
for(i in 1:9){
  cuttoff_list <- cuttoff_list %>% append(foldcount*i)
}

# create function for assigning folds
# really slow needs to be updated
assign_fold <- function(id) {
  if (id<cuttoff_list[1]) {
    return(1)
  } else if (id<=cuttoff_list[2]) {
    return(2)
  } else if (id<=cuttoff_list[3]) {
    return(3)
  } else if (id<=cuttoff_list[4]) {
    return(4)
  } else if (id<=cuttoff_list[5]) {
    return(5)
  } else if (id<=cuttoff_list[6]) {
    return(6)
  } else if (id<=cuttoff_list[7]) {
    return(7)
  } else if (id<=cuttoff_list[8]) {
    return(8)
  } else if (id<=cuttoff_list[9]) {
    return(9)
  } else {
    return(10)
  }
}

set.seed(42)
# generate a random id, no repeated values, from 1 to the size of the data
data.kfold <- data %>% mutate(random_id = sample(1:dim(data)[1], size = dim(data)[1], replace = FALSE), fold = 10) # sample without replacement: https://stackoverflow.com/questions/14864275/randomize-w-no-repeats-using-r

# run assign_fold to get fold assignment from random id, then save as column 
for(i in 1:dim(data.kfold)[1]){
  data.kfold[i,]$fold = assign_fold(data.kfold[i,]$random_id)
}

# need a row id to filter by in KNN section
data.kfold <- data.kfold %>% rowid_to_column()
```

```{r}
data.kfold %>% filter(fold==1) %>% dim()
data.kfold %>% filter(fold==2) %>% dim()
data.kfold %>% filter(fold==3) %>% dim()
data.kfold %>% filter(fold==4) %>% dim()
data.kfold %>% filter(fold==5) %>% dim()
data.kfold %>% filter(fold==6) %>% dim()
data.kfold %>% filter(fold==7) %>% dim()
data.kfold %>% filter(fold==8) %>% dim()
data.kfold %>% filter(fold==9) %>% dim()
data.kfold %>% filter(fold==10) %>% dim()
```

It looks like these folds are evenly split.  There is a strange issue with the first and second fold where the first has 6,323 amd the second has 6,325 when both should have 6,324. Because this is a shift of one observation out of thousands I am not concerned with this error.

## Logistic Regression

```{r logistic-regression, cache=TRUE}
# create function to loop through k folds
cross_val <- function(kfold) {
  # train model on all but the fold passed into the function
  logistic.fit <- glm(Class.cl~Red+Green+Blue,family = binomial, data = data.kfold, subset=fold!=kfold)
  # passed in fold becomes test data
  holdout <- filter(data.kfold, fold == kfold)
  # generate predictions on test data
  predictions <- logistic.fit %>% predict(holdout, type='response')
  # return data frame of actual, then predictions
  return(data_frame(fold=i,actual=holdout$Class.cl,prediction=(1-predictions)))
}

# create empty data frame for prediction data
logistic.pred <- data_frame(fold=numeric(),actual=factor(),prediction=numeric())
# run loop through all folds
for (i in 1:10) {
  logistic.pred<-full_join(logistic.pred, cross_val(i))
}

# calculate the ROC and AUC
library(tidymodels)
# https://yardstick.tidymodels.org/reference/roc_curve.html
logistic.roc <- logistic.pred %>% roc_curve(actual,prediction)
logistic.roc
logistic.auc <- logistic.pred %>% roc_auc(actual,prediction)
logistic.auc

# calculate ideal threshold from ROC
logistic.roc <- logistic.roc %>% rowid_to_column()
logistic.thresh <- logistic.roc %>% filter(sensitivity == 1) %>% arrange(desc(specificity)) %>% head(1)

# plot the ROC
logistic.roc %>% ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() + geom_abline(lty=3) + coord_equal() +
  geom_point(aes(1-logistic.thresh$specificity,logistic.thresh$sensitivity))

# calculate the Precision Recall curve and AUC
logistic.pr <- logistic.pred %>% pr_curve(actual,prediction)
logistic.pr
logistic.pr_auc <- logistic.pred %>% pr_auc(actual,prediction)
logistic.pr_auc

# calculate ideal threshold from PR
logistic.pr <- logistic.pr %>% rowid_to_column()
logistic.thresh.pr <- logistic.pr %>% filter(recall == 1) %>% arrange(desc(precision)) %>% head(1)

# Plot the Precision Recall curve
# Needed some help expanding my graph vertically for readability https://stackoverflow.com/questions/13701347/force-the-origin-to-start-at-0
logistic.pr %>% ggplot(aes(x = recall, y = precision)) +
  geom_path() + coord_equal() + scale_y_continuous(expand = c(0, 0), limits = c(.00,1.05))+
  geom_point(aes(logistic.thresh.pr$recall,logistic.thresh.pr$precision))



```

I calculated logistic regression, LDA and QDA using the same basic code with minor variation. In each I created a function to loop through each of the ten folds, treating the current fold as the test data set and the rest as training.  The function would then calculate predictions for the current fold (the test data) and return the prediction and actuals which were saved in a data frame. The ROC curve was then calculated using the dataframe making it an ROC curve calculated based on all observations with predictions calculated as test data.

For logistic regression, the area under the curve is very close to 1 and the curve itself is very close to reaching the corner. It is clear a wide range of threshold values would be acceptable according to this ROC curve.

Because we have such a small fraction of Blue tarp data, I am concerned that the high volume of correct easy predictions, where we are predicting that this is not a blue tarp, might skew the data. If there is a 97% chance that the observation is not a blue tarp, how can we be sure we are predicting true values (blue tarps) accurately. The ROC curve shows the true positive rate vs the true negative rate; the true negative rate may skew the results. For this reason I decided to also calculate a precision recall curve.

Precision recall show the true positives as a percentage of total positives (recall), versus the true positives as a percentage of all predicted positives (precision). The precision recall also has a very strong AUC, actually stronger than the ROC curve. The curve, as plotted does not have a precision level below 0.95, which is confirmed by the data. The plot suggests that the model is very accurate under most thresholds we capture all true positives and the false positives are not a large proportion.

## LDA
```{r lda, cache=TRUE}
# calculate predictions for specific fold
lda.cross_val <- function(kfold) {
  lda.fit <- lda(Class.cl~Red+Green+Blue, data = data.kfold, subset=fold!=kfold)
  holdout <- filter(data.kfold, fold == kfold)
  predictions <- lda.fit %>% predict(holdout, type='response')
  # return actual, then predictions
  # Predictions here are a little more complicated than in logistic regression. We needed to specify the second column of posterior predictions.
  return(data_frame(fold=i,actual=holdout$Class.cl,prediction=predictions$posterior[,2],pred_class=predictions$class, x=predictions$x))
}

# perform cross validation by looping through all folds and combining predictions
lda.pred <- data_frame(fold=numeric(),actual=factor(),prediction=numeric(), pred_class=factor(), x=numeric())
for (i in 1:10) {
  lda.pred<-full_join(lda.pred, lda.cross_val(i))
}

# https://yardstick.tidymodels.org/reference/roc_curve.html
# calculate ROC
lda.roc <- lda.pred %>% roc_curve(actual,prediction, event_level='second') # we need to specify that we are predicting the second variable
lda.roc
lda.auc <- lda.pred %>% roc_auc(actual,prediction, event_level='second')
lda.auc

# calculate ideal threshold from ROC
lda.roc <- lda.roc %>% rowid_to_column()
lda.thresh <- lda.roc %>% filter(sensitivity == 1) %>% arrange(desc(specificity)) %>% head(1)

# plot ROC
lda.roc %>% ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() + geom_abline(lty=3) + coord_equal() +
  geom_point(aes(1-lda.thresh$specificity,lda.thresh$sensitivity))

# calculate PR
lda.pr <- lda.pred %>% pr_curve(actual,prediction, event_level='second')
lda.pr
lda.pr_auc <- lda.pred %>% pr_auc(actual,prediction, event_level='second')
lda.pr_auc

# calculate ideal threshold from PR
lda.pr <- lda.pr %>% rowid_to_column()
lda.thresh.pr <- lda.pr %>% filter(recall == 1) %>% arrange(desc(precision)) %>% head(1)

# plot PR
lda.pr %>% ggplot(aes(x = recall, y = precision)) +
  geom_path() + coord_equal() +
  geom_point(aes(lda.thresh.pr$recall,lda.thresh.pr$precision))
```
LDA follows logistic regression model calculation very closely. The only major difference being the output from lda.prediction. However, with minor adjustment the outcome is the same.

The ROC curve is not as strong on LDA as it is on Logistic regression and the precision recall curve shows a sharp drop off in performance. Because we are concerned with the small number of true values in the data, we will weight the precision recall curve more heavily.

## QDA
```{r qda, cache=TRUE}
# calculate predictions for current fold
qda.cross_val <- function(kfold) {
  qda.fit <- qda(Class.cl~Red+Green+Blue, data = data.kfold, subset=fold!=kfold)
  holdout <- filter(data.kfold, fold == kfold)
  predictions <- qda.fit %>% predict(holdout, type='response')
  # return actual, then predictions
  # data returned from predictions is similar to LDA
  return(data_frame(fold=i,actual=holdout$Class.cl,prediction=predictions$posterior[,2]))
}

# perform cross validation and gather predictions accross folds
qda.pred <- data_frame(fold=numeric(),actual=factor(),prediction=numeric())
for (i in 1:10) {
  qda.pred<-full_join(qda.pred, qda.cross_val(i))
}

# https://yardstick.tidymodels.org/reference/roc_curve.html
# calculate ROC
qda.roc <- qda.pred %>% roc_curve(actual,prediction, event_level='second')
qda.roc
qda.auc <- qda.pred %>% roc_auc(actual,prediction, event_level='second')
qda.auc

# calculate ideal threshold from ROC
qda.roc <- qda.roc %>% rowid_to_column()
qda.thresh <- qda.roc %>% filter(sensitivity == 1) %>% arrange(desc(specificity)) %>% head(1)

# plot ROC
qda.roc %>% ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() + geom_abline(lty=3) + coord_equal() +
  geom_point(aes(1-qda.thresh$specificity,qda.thresh$sensitivity))

# calculate PR
qda.pr <- qda.pred %>% pr_curve(actual,prediction, event_level='second')
qda.pr
qda.pr_auc <- qda.pred %>% pr_auc(actual,prediction, event_level='second')
qda.pr_auc

# calculate ideal threshold from PR
qda.pr <- qda.pr %>% rowid_to_column()
qda.thresh.pr <- qda.pr %>% filter(recall == 1) %>% arrange(desc(precision)) %>% head(1)
qda.thresh.pr.hp <- qda.pr[2600,]

# plot PR
qda.pr %>% ggplot(aes(x = recall, y = precision)) +
  geom_path() + coord_equal() +
  geom_point(aes(qda.thresh.pr$recall,qda.thresh.pr$precision)) +
  geom_point(aes(qda.pr[2600,]$recall,qda.pr[2600,]$precision, colour = 'red'))

```

The calculation of QDA is almost identical to LDA. The ROC results are closer to Logistic regression with only a slightly lower AUC.  The precision recall out performs LDA but is behind Logistic regression. Because precision recall and ROC appear significantly different, we will give preference to precision recall as stated above.

## KNN
```{r, knn, cache=TRUE}
# use my 10 folds for use in caret::train:
# https://stackoverflow.com/questions/48458407/cross-validation-predictions-from-caret-in-assigned-to-different-folds
getFoldList <- function(i){
  fold <- data.kfold %>% filter(fold==i)
  fold$rowid
}
fold_list <- 1:10 %>% purrr::map(getFoldList)

# fit model
ctl <- trainControl(method = "cv", number = 10, index = fold_list)
knn.fit <- caret::train(Class.cl~Red+Green+Blue, data = data.kfold, method = 'knn', trControl = ctl)
knn.fit

# calculate predictions on data using best model
knn.pred <- knn.fit %>% predict(data.kfold, type='prob')
data.knn <- data.kfold %>% mutate(prediction = knn.pred[,1])

# calculate ROC
knn.roc <- data.knn %>% roc_curve(Class.cl,prediction)
knn.roc
knn.auc <- data.knn %>% roc_auc(Class.cl,prediction)
knn.auc

# calculate ideal threshold from ROC
knn.roc <- knn.roc %>% rowid_to_column()
knn.thresh <- knn.roc %>% filter(sensitivity == 1) %>% arrange(desc(specificity)) %>% head(1)

# plot ROC
knn.roc %>% ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() + geom_abline(lty=3) + coord_equal() +
  geom_point(aes(1-knn.thresh$specificity,knn.thresh$sensitivity))

# calculate PR
knn.pr <- data.knn %>% pr_curve(Class.cl,prediction)
knn.pr
knn.pr_auc <- data.knn %>% pr_auc(Class.cl,prediction)
knn.pr_auc

# calculate ideal threshold from PR
knn.pr <- knn.pr %>% rowid_to_column()
knn.thresh.pr <- knn.pr %>% filter(recall == 1) %>% arrange(desc(precision)) %>% head(1)

# plot PR
knn.pr %>% ggplot(aes(x = recall, y = precision)) +
  geom_path() + coord_equal() + scale_y_continuous(expand = c(0, 0), limits = c(.00,1.05)) +
  geom_point(aes(knn.thresh.pr$recall,knn.thresh.pr$precision))

```

For KNN we used the Caret package. Caret gives us a lot of flexibility and allows us to pass in our current folds so that we can compare this model with the others. The ROC curve and precision recall curve appears to outperform even logistic regression.

### Tuning Parameter $k$

```{r knn-tuning}
knn.fit$finalModel
knn.fit$bestTune
```

The Caret package will run the cross validation through multiple values of k to select the optimal model. The model best model is the final model shown above, with the best tune of k=5. This is the model we used to calculate the ROC curve and precision recall curve.

## Penalized Logistic Regression (ElasticNet)

```{r, eval=FALSE, echo=FALSE}
# tuning <- data.frame(alpha=c(rep(0,100),rep(.5,100),rep(1,100)), lambda=10^seq(10,0,length=300))
# elnet.fit <- caret::train(Class.cl~Red+Green+Blue, data=data.kfold, method="glmnet", 
#              tuneGrid=tuning, # alpha 0 is ridge alpha 1 is lasso
#              trControl=trainControl("cv", number=5, returnResamp='all', index = fold_list, search = "grid"))
```

```{r elasticnet, cache=TRUE}
# First method
# loop through lambda and alpha
library(glmnet)
set.seed(42)


# Second method
# https://stackoverflow.com/questions/48280074/r-how-to-let-glmnet-select-lambda-while-providing-an-alpha-range-in-caret
elnet.fit <- caret::train(Class.cl~Red+Green+Blue, data=data.kfold, method="glmnet", 
             #tuneGrid=data.frame(alpha=rep(1,100), # alpha 0 is ridge alpha 1 is lasso
                                 #lambda=10^seq(10,0,length=100)),  # selected lambda range so that full curve is displayed
             trControl=trainControl("cv", number=5, returnResamp='all', index = fold_list, search = "grid"))

# calculate predictions on data using best model
elnet.pred <- elnet.fit %>% predict(data.kfold, type='prob', s=elnet.fit$bestTune[2])

data.elnet <- data.kfold %>% mutate(prediction = elnet.pred[,2])

# calculate ROC
elnet.roc <- data.elnet %>% roc_curve(Class.cl,prediction, event_level='second')
elnet.roc
elnet.auc <- data.elnet %>% roc_auc(Class.cl,prediction, event_level='second')
elnet.auc

# calculate ideal threshold from ROC
elnet.roc <- elnet.roc %>% rowid_to_column()
elnet.thresh <- elnet.roc %>% filter(sensitivity == 1) %>% arrange(desc(specificity)) %>% head(1)

# plot ROC
elnet.roc %>% ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() + geom_abline(lty=3) + coord_equal() +
  geom_point(aes(1-elnet.thresh$specificity,elnet.thresh$sensitivity))

# calculate PR
elnet.pr <- data.elnet %>% pr_curve(Class.cl,prediction, event_level='second')
elnet.pr
elnet.pr_auc <- data.elnet %>% pr_auc(Class.cl,prediction, event_level='second')
elnet.pr_auc

# calculate ideal threshold from PR
elnet.pr <- elnet.pr %>% rowid_to_column()
elnet.thresh.pr <- elnet.pr %>% filter(recall == 1) %>% arrange(desc(precision)) %>% head(1)
elnet.thresh.pr.hp <- elnet.pr[1900,]

# plot PR
elnet.pr %>% ggplot(aes(x = recall, y = precision)) +
  geom_path() + coord_equal() +
  geom_point(aes(elnet.thresh.pr$recall,elnet.thresh.pr$precision)) +
  geom_point(aes(elnet.pr[1900,]$recall,elnet.pr[1900,]$precision, colour = 'red'))


```

### Tuning Parameters

Elastic net has two tuning pararmeters, alpha and lambda. !!!!!!!!!!!!!!!!!!!!!

**NOTE: PART II same as above plus add Random Forest and SVM to Model Training.**

## Threshold Selection

Up until this point we have evaluate models broadly but have not selected any thresholds. Threshold selection requires us to make some assumptions. Because we have limited information about the resources to handle these supply deliveries and rescues, or the priorities of the vulnerable people or the helpers on the ground, we will have to make assumptions on our own and recognize that with information from people closer to the situation, these assumptions will change.

My hope is that this disaster received enough worldwide support that resources are abundant and that the key limiting factor from delivering help is a lack of information. With this assumption in place, we will target a lower threshold that will lean toward classifying pixels as blue tarp pixels when in doubt. In other words we are trying to be as efficient as possible with a no person left behind approach. This means that we will shoot for the highest possible true positive rate and the highest recall and sensitivity as our first priority, while maximizing precision and selectivity under these conditions.

To achieve this, I performed the same task for each model. On the ROC data, I filtered for a sesitivity of 1 (the maximum value), and then selected the threshold with the highest specificity from the list. For the precision, recall data, I filtered by a recall of 1 and then selected the threshold for the highest precision from the list.

Finally, from each model I compared the threshold calculated by the ROC data and the precision recall data and selected the better fit.

# Results (Cross-Validation)

CV Performance Table based on the threshold obtained using the ROC Curve

```{r echo = FALSE, results='asis'}
library(knitr)

# calculate accuracy
getAcc <- function(prob, tr, actual){
  ind <- which(prob > tr)
  predBlue <- actual[ind]
  truePos <- table(predBlue)["Blue Tarp"]
  predOther <- actual[-ind]
  trueNeg <- table(predBlue)["Other"]
  acc <- (truePos + trueNeg)/length(actual)
  acc
}

logistic.acc <- getAcc(logistic.pred$prediction,logistic.thresh$.threshold,logistic.pred$actual)
lda.acc <- getAcc(lda.pred$prediction,lda.thresh$.threshold,lda.pred$actual)
qda.acc <- getAcc(qda.pred$prediction,qda.thresh$.threshold,qda.pred$actual)
knn.acc <- getAcc(data.knn$prediction,knn.thresh$.threshold,data.knn$Class.cl)
elnet.acc <- getAcc(data.elnet$prediction,elnet.thresh$.threshold,data.elnet$Class.cl)

# calculate TPR
# condPos <- data.kfold %>% filter(Class.cl == "Blue Tarp") %>% count()
condNeg <- data.kfold %>% filter(Class.cl == "Other") %>% count()
getFPR <- function(prob, tr, actual){
  ind <- which(prob > tr)
  predBlue <- actual[ind]
  falsePos <- table(predBlue)["Other"]
  falsePosRate <- falsePos/condNeg
  falsePosRate$n
}

logistic.FPR <- getFPR(logistic.pred$prediction,logistic.thresh$.threshold,logistic.pred$actual)
lda.FPR <- getFPR(lda.pred$prediction,lda.thresh$.threshold,lda.pred$actual)
qda.FPR <- getFPR(qda.pred$prediction,qda.thresh$.threshold,qda.pred$actual)
knn.FPR <- getFPR(data.knn$prediction,knn.thresh$.threshold,data.knn$Class.cl)
elnet.FPR <- getFPR(data.elnet$prediction,elnet.thresh$.threshold,data.elnet$Class.cl)

# pull precision for ROC threshold from PR table
logistic.Prec.ROC <- logistic.pr %>% filter(.threshold == logistic.thresh$.threshold)
lda.Prec.ROC <- lda.pr %>% filter(.threshold == lda.thresh$.threshold)
qda.Prec.ROC <- qda.pr %>% filter(.threshold == qda.thresh$.threshold)
knn.Prec.ROC <- knn.pr %>% filter(.threshold == knn.thresh$.threshold)
elnet.Prec.ROC <- elnet.pr %>% filter(.threshold == elnet.thresh$.threshold)

df <- data.frame(
  Model=c("Logistic Reg","LDA","QDA","KNN","Elastic Net"),
  Tuning=c(NA,NA,NA,paste("k: ",knn.fit$results[1,1]),paste("alpha: ",elnet.fit$results[1,1],"lambda: ",elnet.fit$results[1,2])),
  AUROC=c(logistic.auc$.estimate,lda.auc$.estimate,qda.auc$.estimate,knn.auc$.estimate,elnet.auc$.estimate),
  Threshold=c(logistic.thresh$.threshold,lda.thresh$.threshold,qda.thresh$.threshold,knn.thresh$.threshold,elnet.thresh$.threshold),
  Accuracy=c(logistic.acc,lda.acc,qda.acc,knn.acc,elnet.acc), 
  TPR=c(logistic.thresh$sensitivity,lda.thresh$sensitivity,qda.thresh$sensitivity,knn.thresh$sensitivity,elnet.thresh$sensitivity),
  FPR=c(logistic.FPR,lda.FPR,qda.FPR,knn.FPR,elnet.FPR),
  Precision=c(logistic.Prec.ROC$precision,lda.Prec.ROC$precision,qda.Prec.ROC$precision,knn.Prec.ROC$precision,elnet.Prec.ROC$precision)
)
kable(df)

```

CV Performance Table using the threshold obtained through the Precision Recall curve

```{r echo = FALSE, results='asis'}

# logistic.TPR.pr <- logistic.roc %>% filter(.threshold == logistic.thresh.pr$.threshold)
# lda.TPR.pr <- lda.roc %>% filter(.threshold == lda.thresh.pr$.threshold)
# qda.TPR.pr <- qda.roc %>% filter(.threshold == qda.thresh.pr$.threshold)
# knn.TPR.pr <- knn.roc %>% filter(.threshold == knn.thresh.pr$.threshold)
# elnet.TPR.pr <- elnet.roc %>% filter(.threshold == elnet.thresh.pr$.threshold)


# estimate acc
logistic.acc.pr <- getAcc(logistic.pred$prediction,logistic.thresh.pr$.threshold,logistic.pred$actual)
lda.acc.pr <- getAcc(lda.pred$prediction,lda.thresh.pr$.threshold,lda.pred$actual)
qda.acc.pr <- getAcc(qda.pred$prediction,qda.thresh.pr.hp$.threshold,qda.pred$actual)
knn.acc.pr <- getAcc(data.knn$prediction,knn.thresh.pr$.threshold,data.knn$Class.cl)
elnet.acc.pr <- getAcc(data.elnet$prediction,elnet.thresh.pr.hp$.threshold,data.elnet$Class.cl)

# estimate FPR
logistic.FPR.pr <- getFPR(logistic.pred$prediction,logistic.thresh.pr$.threshold,logistic.pred$actual)
lda.FPR.pr <- getFPR(lda.pred$prediction,lda.thresh.pr$.threshold,lda.pred$actual)
qda.FPR.pr <- getFPR(qda.pred$prediction,qda.thresh.pr.hp$.threshold,qda.pred$actual)
knn.FPR.pr <- getFPR(data.knn$prediction,knn.thresh.pr$.threshold,data.knn$Class.cl)
elnet.FPR.pr <- getFPR(data.elnet$prediction,elnet.thresh.pr.hp$.threshold,data.elnet$Class.cl)

# note: elnet and qda have handpicked thresholds
df2 <- data.frame(
  Model=c("Logistic Reg","LDA","QDA","KNN","Elastic Net"),
  Threshold=c(logistic.thresh.pr$.threshold,lda.thresh.pr$.threshold,qda.thresh.pr.hp$.threshold,knn.thresh.pr$.threshold,elnet.thresh.pr.hp$.threshold),
  Accuracy=c(NA,NA,NA,knn.fit$results[1,2],elnet.fit$results[1,3]), ##########
  TPR=c(logistic.thresh.pr$recall,lda.thresh.pr$recall,qda.thresh.pr.hp$recall,knn.thresh.pr$recall,elnet.thresh.pr.hp$recall),
  FPR=c(logistic.FPR.pr,lda.FPR.pr,qda.FPR.pr,knn.FPR.pr,elnet.FPR.pr), 
  Precision=c(logistic.thresh.pr$precision,lda.thresh.pr$precision,qda.thresh.pr$precision,knn.thresh.pr$precision,elnet.thresh.pr$precision)
)

kable(df2)
```

# Conclusions

### Conclusion \#1 - Model Selection

In selecting this model I want to emphasize the weakness of our assumptions. For this model we selected a threshold assuming unlimited resources. In practice, I recommend starting with this model and the blue tarp predictions it gives initially. If this number of blue tarps is unfeasible for the amount of resources you have, increase the threshold so that there are fewer blue tarps predicted so that they can be investigated and supported with the resources you have. This will reduce the chances that resources are wasted on false positives, while ensuring that the higest number of true positives are served.

Finally, I encourage users of this model to decide ahead of time what percentage of resources will be allocated by this model and which will be allocated using more traditional means of information gathering. No model is perfect and should not be treated as such. Allocating resources to the data provided by this model ahead of analysing model output will prevent the instinct to chase false positives.

### Conclusion \#2

??????? Why does QDA outperform LDA but not Logistic (is logstic performing well or QDA?)

### Conclusion \#3


```{r, echo=FALSE}
knitr::knit_exit()    # ignore everything after this
## Uncomment this line for Part I
## You can remove the entire code chunk for Part II
```


# Hold-out Data / EDA

Load data, explore data, etc. 


# Results (Hold-Out)

**Hold-Out Performance Table Here**


# Final Conclusions

### Conclusion \#1 

### Conclusion \#2

### Conclusion \#3

### Conclusion \#4 

### Conclusion \#5

### Conclusion \#6
